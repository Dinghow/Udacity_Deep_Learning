{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originate'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295918 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "xiltbmwscxcnefit wlgnjkatct jgow  r nbzeslaki  rt onnl ro jfpz i  aercerzh eugxj\n",
      "nsohtstnnlzcy  a gnagujci beyat  x ztdiercl nv  e hj g itusrmzom efzziuw   oz rc\n",
      "nniaae nnfnvfarqefdan irfuz fzcxevcgtairwagitanid  jubo dk iwuv sljnm tyiutvn rp\n",
      "x ar tkc dtwxafeym  t icjhus srptdndrnyr wb wvawd nixaphtpkrjx c  rfn w bb r  vi\n",
      "ztpfs  upo w itn xcum y aiejfoiajsryi b ieeni qiieaxsexrpdsbtz hreztiaeenjzoyanl\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.596021 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.84\n",
      "Validation set perplexity: 10.23\n",
      "Average loss at step 200: 2.251585 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.79\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 300: 2.099724 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.000765 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 500: 1.934310 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.905237 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 700: 1.855778 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 800: 1.819117 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 900: 1.829557 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1000: 1.825140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "tyegh esprice in one nine nine miny pokn fralmused lite the bass cilater heild o\n",
      "cleden from tensd tefent earuh copons licessiend with of a musuruce in muricad u\n",
      "zoray comsjefcred in one ond nuter and rcons seave brougn aire the gengrouhe by \n",
      "hate kifforister hewt of sumenion s liection e one zero elesperion s cluting cun\n",
      "qudief be edisu pretcery bation prodegs b crophisy gavp deato in their eldecume \n",
      "================================================================================\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1100: 1.774942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1200: 1.753551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.732468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.743852 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1500: 1.734995 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1600: 1.746255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.711873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.677690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.645106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2000: 1.695011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "ass salg one nine eight zero one nine seven gameng gron he so im shew and name g\n",
      "ant profict ssud of is than perpoose on indicani is to bastable polutticumdy the\n",
      "plosier jomans non that new a gosconta is indlaping spages of ro midua taken be \n",
      "ition chared perlins in const the ordeszers seven and act of proviche speen from\n",
      "e shah mashornzles speck number huohn since three jamist or lynss of hiy suelo g\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.682162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.675879 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300: 1.641313 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2400: 1.660334 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2500: 1.678075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2600: 1.652569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2700: 1.654666 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2800: 1.645309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.646868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3000: 1.650240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "joshibns may is sociation is bilthony and party indipanitionations in the prodsp\n",
      "y eigntly fiel comples arroun obtage fincing with and suptee theora and its asco\n",
      "x becewide in ejchan may these to such they secpines and bringles zerien of forg\n",
      "me buddament an emperot opigning petall a the five ks stands countibe supanes li\n",
      "fics feoterged liberidical functo are and a isly in shortually the the lefes con\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100: 1.630593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200: 1.645692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3300: 1.633488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.663410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3500: 1.652996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.668938 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.645375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3800: 1.639764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3900: 1.636100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.648235 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "king uninide gaverie the by sodess chick utire the king to goon of but beating t\n",
      "med architide verice were corresor grobittinations nambine weiral panit abcurel \n",
      "tels to d simple bibtht ca takewen populially liim of nations allects nottend is\n",
      "bined repusay varing popper war forces wryugh no cangize antiron are photector g\n",
      "gay rated are dise genefbers ig where cuals ceve dypemate fuel provisition there\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.628006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.638256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4300: 1.610828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4400: 1.604956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4500: 1.615460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.614482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.627700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4800: 1.632806 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4900: 1.634851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5000: 1.608837 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "kersing to invil sahper p other penlelder seven sudents norting of their surnati\n",
      "keng tavary granmast for prefises the migration one six zero zero a liuns him ha\n",
      "ple dypter savide is sholleting prepekies more or wenther grand s a eight th con\n",
      "cily received in dese the a burn from a mila is with passe or dewont grook amorb\n",
      "n orepants gave iras a ii netwought interfation or b suchur alvovers eogine forp\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.603011 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5200: 1.587018 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5300: 1.577823 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5400: 1.574302 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5500: 1.564056 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5600: 1.580039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5700: 1.567389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5800: 1.577568 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.576804 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6000: 1.542579 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "k national malieg alfond in g a compression is muntul wenk ramabo guok numbers t\n",
      "d from conceins ambise it the frang the embera lord penso counce tends themen th\n",
      "itifacy also the study nan tracta phymole derassor get remendent experove bs chi\n",
      "ap and made four contractions the world with hollists encide from the sancholvin\n",
      "walsming megm thue oclainous a mode a application in the lists between may in fo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6100: 1.567372 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6200: 1.533457 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6300: 1.544626 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6400: 1.536898 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6500: 1.555436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6600: 1.598497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6700: 1.582377 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6800: 1.601428 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6900: 1.582153 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7000: 1.573706 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "zanesh for hiz now be of its leughandic screalor only in there is nicumbored and\n",
      "zers attack bilagiration a crousuble refer pracrively united seetical is the cro\n",
      "wor on engavanes of american and have which to beles and underewe develops and d\n",
      "y orbide lybakes of ettles clasing beciend for presorytherstumentous rostically \n",
      "entaiis and on the protect to pandera which to the concentive mamm beart sigulah\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "jx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "zx = tf.concat([tx, jx],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(27), Dimension(128)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenated matrix\n",
    "  sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "  sm = tf.concat([im, fm, cm, om], 1)\n",
    "  sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    smultiple = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    input_d, forget_d, update, output_d = tf.split(smultiple, 4, 1)\n",
    "    input_gate = tf.sigmoid(input_d)\n",
    "    forget_gate = tf.sigmoid(forget_d)\n",
    "    output_gate = tf.sigmoid(output_d)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294599 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "w fcchpceyn m  eh usfycarghia  nyvghelybi z eptne lm eeefgr wvuarl yisjbyrrch bk\n",
      "zzvir t le  ehhjs mj vrnlpajazoijsezoxr esdn jnfzleg me bcpe r uidh sxxeigjcspio\n",
      "xnhra pescp cxphmz eeuounyvhfxcxemsh qsh efrebsmwool mr teqlrm sk a lmryet tdvow\n",
      "u kwhy  v orcskaeesu jfiodo  bc cdxtuapizv izlmwlji pscysreagyumume meufsroaxyam\n",
      "likqv rnhfhb wf cabkaehropwscx   e x sfnabjxzcihxnrnr rd erjs bkbepegyoq crprqur\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 2.576622 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.34\n",
      "Validation set perplexity: 10.71\n",
      "Average loss at step 200: 2.246409 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.54\n",
      "Validation set perplexity: 8.94\n",
      "Average loss at step 300: 2.086570 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 400: 2.035812 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 500: 1.983019 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 600: 1.896463 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 700: 1.872424 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 800: 1.870503 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 900: 1.845226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 1000: 1.848441 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "================================================================================\n",
      "astict midion loptions two his maclone to rethoughy one eight revine edchoron vi\n",
      "y sakivene jom blove tullan notman melicicy maciainion deplimol list heoplatovio\n",
      "pers fied is a one eight two lig three zero seven one six zelo one seven nine ci\n",
      "jegn sufwerent wroppents prose usiligumants a ruging ponsan whiel and be playant\n",
      "remed lopercone from lelrametig was sin usens invered and k plebsions prafmable \n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.800904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1200: 1.771581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1300: 1.763335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1400: 1.766487 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.752458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1600: 1.736470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.720548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1800: 1.693392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1900: 1.699162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2000: 1.683125 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "flicia mace obective in tocolat seforely foum long ritver two most of clap defun\n",
      "puration fvrance of incest call conficted zero nazoph enging the unitimating for\n",
      "ver feaged troll rear four thmes brouga tiance averectatious your force isseed f\n",
      "dival issaents and he aract are ralended the bectron comparit is batter panterct\n",
      "quious godsed anoutt iseven two zero zero five five four linchisistion mactern y\n",
      "================================================================================\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2100: 1.689310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2200: 1.711088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2300: 1.712307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2400: 1.690357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2500: 1.697384 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2600: 1.678157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2700: 1.683573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2800: 1.686135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2900: 1.679529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 3000: 1.688408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "us work fimany in the lim fictron a che memoral s sirped diefers laurknnated if \n",
      "wing rea add badi wats tentwany the sawers became mym d and desteme for the b co\n",
      "t commet duri and convor purant off the paition boty a no frences feels jany is \n",
      " prastly its particut alsysters laurub and other viscn gernation been no colinen\n",
      "h some the afuey thara lowt number corectury thirsh in suck corphanger to fromed\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3100: 1.657641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3200: 1.636363 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3300: 1.649191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3400: 1.641356 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3500: 1.676002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3600: 1.656605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3700: 1.654817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.658603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.652609 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4000: 1.645080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "stany electery fians was emm medies of s stree of the incordication the director\n",
      "des was and s subing ob uncounding of from nails two nine a one five marination \n",
      "eller invere sophable art peesland been centrocesmits amerieifium of one zero mu\n",
      "ong and network mankrantern from industro moints of the peavi when sultual phill\n",
      "rechauations a noter omp has winstay stase leader ba whrt afting was combasish c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4100: 1.623973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4200: 1.617198 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4300: 1.623570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4400: 1.611446 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4500: 1.646730 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4600: 1.623779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4700: 1.625710 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4800: 1.610068 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4900: 1.621098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.613239 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "n also tranch wompo cannomies uniness leasing the eightin from diolest is one ni\n",
      "d secharenful popuacition mosery of the times the accewning confeal serse imstar\n",
      "flemally rocaical supplinently the being the humandes eight the ainnats they of \n",
      "ground on the muss metisthered the paplaill of electroned of tenes a certerian f\n",
      "ble player whild the treates betweren tancia one the one nine three zero morring\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5100: 1.593116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5200: 1.594635 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5300: 1.598741 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5400: 1.594149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.592243 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.564741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.577735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5800: 1.601673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5900: 1.579449 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6000: 1.586094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "file grier of nect to fame and the defictively in the coduction manding was the \n",
      "file s debatamey batt seas as user game complete of the plays for bering work ar\n",
      "thandent quilfieties minsing kanged northes attement because to the also biel ha\n",
      "ch saurces hile insiit of competer s moders malpohals was mussige relaced and re\n",
      "ising ulturian carchies to speaked repredine logillk afria defincts that thus ac\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6100: 1.582006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.588377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6300: 1.588337 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.572951 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6500: 1.557475 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6600: 1.595919 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6700: 1.573588 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.575948 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6900: 1.572709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.590542 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "zed to be g in through stoadious its tu are europtica pronist musing such a fiti\n",
      "wend as metracital million present to model ane as sive catucial a methogame in \n",
      "prince by has ser and english is one nine seven four one nine seven two five fiv\n",
      "nadionies by john as calard its work addister of esple pmin chocutin excess it o\n",
      "k follony centurution tho bytgeak a put revel thestook to projown from mons amer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question a: let's add an embedding layer for the input chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding layer: input, embedding output\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295465 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "br mt oohuinpt nmrmg aer r c g rii  ne rk hug u xufe xubc kao ss siq zkaqh op mc\n",
      "m ust oey  oa hn evyr ec lnspi hxast q bft nfnunhouzfe ot q t vbznsepi   iminiy \n",
      "veed d d d c hfk hwkse h nelxf e yqhzq eq mohhi ohj o  ggt kit  ownn ykx khn  on\n",
      "zwx oyunt spneit  mjrhwr befaeper s poea a ot a od qobd k jit q woyg tbzqse m  t\n",
      "t vu d d sd xbkrubkn t a uafpmud leqefye qiao  wt jvr c c nhf  neg ahx oasm fq  \n",
      "================================================================================\n",
      "Validation set perplexity: 23.33\n",
      "Average loss at step 100: 2.307625 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.21\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 200: 2.020914 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 300: 1.917944 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 400: 1.865545 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 500: 1.884436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 600: 1.818542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 700: 1.801601 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 800: 1.789184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 900: 1.783294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1000: 1.718883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "de of orits is theur fie baft or domes pribal of resoucur in three one nine nine\n",
      "z of herent l oral in as sinkent is on paprian enw prous nowical modered maritoo\n",
      "ject or multter feguagence occuall was is recend one one niz nine nine sepenger \n",
      "king or usek of lesbon king eler braditimon one nine six dosy mellon wokports sb\n",
      "wer a recologind the paputer one nine nine ninyn usend or the con one six one ni\n",
      "================================================================================\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1100: 1.701226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1200: 1.730022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1300: 1.709193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1400: 1.688631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1500: 1.678102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1600: 1.675644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1700: 1.702217 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1800: 1.667400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1900: 1.675555 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2000: 1.689095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "fered estrest pain one four the occitlate or all boadha wich one three deconssio\n",
      "ian would asife goable statedly mer songese langutare shorte the cungs avame sys\n",
      "wher a a poent forly navir babled boblisoped transsable manumm bischections to t\n",
      "y compament to the way of colling of the corn degions of stated with four three \n",
      "ated sequence kinguac lo did dee on create was telatae convents country murcy br\n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2100: 1.678033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2200: 1.649423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2300: 1.661013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2400: 1.661305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2500: 1.688313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2600: 1.655470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2700: 1.671900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2800: 1.633287 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2900: 1.643915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3000: 1.648542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "usefts by marrities chicizal cause common domour setition to releary with asqale\n",
      " about acewi and word is receids corperts is languages is com the used islantent\n",
      "y of the purds remore knightly the adugh isofting usefuly the rack produce in on\n",
      "possing countined donaminiencs are up a opero b doou any phoyed in kene name adv\n",
      "ire thoremin gestly brita manicallase divitical somesual centrated inclutiber ad\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3100: 1.645234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3200: 1.646900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3300: 1.623725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3400: 1.628908 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.623440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.624935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3700: 1.626293 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3800: 1.621598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3900: 1.618282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4000: 1.621810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "y aelling decessions plicey akeris mutricidy the and rexdorser form line hompary\n",
      "a didau en they speased on interrese jabaticism baftil moration highs v is of th\n",
      "er thruze islams and al five the four setters allowing sname ebrougue an arming \n",
      "onfined usst rong the breasonnary of restructure the orting rabual zacosas arest\n",
      "chai of nuchelbk debberting a pear succescub oq thg tirshoption specitic ontiter\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.620088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4200: 1.608747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4300: 1.594668 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4400: 1.625787 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4500: 1.635996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4600: 1.635203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4700: 1.612105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4800: 1.594385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4900: 1.608753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5000: 1.631268 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "tant libiois unditional repla from equiptity prummits micch offten the compority\n",
      "batmany electaer in becaur anatuless zame bnarkh election there or includiad wor\n",
      "jalions and to gians as won the steven two quast alvant types siletch en justpit\n",
      "ings called edition wholibrits at borces s newah ra air for from the cisce al am\n",
      " cubler is law two five zero milf one eight one six two zero one nine five two g\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5100: 1.619791 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5200: 1.607702 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.568921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.565541 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500: 1.560614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5600: 1.585068 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5700: 1.543853 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.88\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5800: 1.549986 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5900: 1.566101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6000: 1.534456 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "an well change often marding a slavined car allaher to its that impenumersia the\n",
      "borger the secring the cause in common metwoocental code to ration air russian b\n",
      "ject and one eight five sound same bebilg hists is like bagmedn this the genamen\n",
      "jox and sometix of itent for one of a cleause the rajn was exyms arquists of pro\n",
      "ward unived and loss in the filke guancia of time sourhera the does the as repub\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6100: 1.554836 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6200: 1.572899 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6300: 1.585058 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6400: 1.612855 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6500: 1.612339 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.578042 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6700: 1.566084 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6800: 1.551386 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6900: 1.540082 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.559413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "a the civing the socive s in the untaion of could operating stones those signed \n",
      "formed but with between wonzed rhoythern directed a only election comple nationa\n",
      "portant his formull f biologing their referiel also fact skile lfffer broby on t\n",
      "dian accolited activity screass of bot the railed norce yousti fead see know ret\n",
      "rian s serven of four eight seven eran one nine six his then entarifance ontiner\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question b: write a bigram-based LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding layer: input, embedding output\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  single_char_inputs = train_data[:num_unrollings]\n",
    "  train_inputs = zip(single_char_inputs[:-1], single_char_inputs[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    index = tf.argmax(i[0], dimension=1) + tf.argmax(i[1], dimension=1) * vocabulary_size\n",
    "    embed = tf.nn.embedding_lookup(embeddings, index)\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  sample_input_index = tf.argmax(sample_input[0], dimension=1) + tf.argmax(sample_input[1], dimension=1) * vocabulary_size\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301809 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.16\n",
      "================================================================================\n",
      "ssnvyvg   nunianb xgsipis tta duymrzl cwettrsdtmq   w sb j elfxt thesmgkia gkahdm\n",
      "qkmoyeehaimtsi etiwer iih b ric  exgdvrrpvn um rj cnycnio anrovugwertavjovelbbtwf\n",
      "iuus jkswaeieqbpl ekrio ixdpyjfbu  n afdscvxbonlx  mvy jnu s u wzq f kolagefsmnis\n",
      "trr vagbqsk eeetrtjeekxiiueta kzpo bfctpi urj p q icugewzazidvsav  awe trvoto ebc\n",
      "dccnvp w eoorubitvojbujwraaeoux xoweytwaarb sqmpkqjtd fituh mroenhvkdnzenuz bdepm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.38\n",
      "Average loss at step 100: 2.277014 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 200: 1.973701 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 300: 1.885835 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 400: 1.828601 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.763673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 600: 1.758395 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 700: 1.739094 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 800: 1.723634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 900: 1.718333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1000: 1.687468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "fqdocaps  realestal jetorials denstillegel implexidentillion copyues of operal so\n",
      "hy mainst musion s what one ne his some sime may and was against needer and c him\n",
      "y and intebnta the converions descep years were chansike cotper of the forment so\n",
      "als the bleb lsing it to their ution intelle introll outple name king boused by s\n",
      "bqfuke in form the stopsine stanction autizedois nother into ard the unition tost\n",
      "================================================================================\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1100: 1.693351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1200: 1.692988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1300: 1.690797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 1400: 1.660049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 1500: 1.648113 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 1600: 1.636136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 1700: 1.652110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1800: 1.666049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1900: 1.652468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2000: 1.663798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "pge of as more one nine eight one ph one nine six six a singly one seven zero feo\n",
      "hppekit shigency unimature you unander that the monica rezums generabel which how\n",
      "renoment movement machine rehangeogne new yors brittac year grantion of an etitis\n",
      "cm one zero s diefs arpoinism bii formed and university the form azartanily cd ch\n",
      "pbor also deferentited war hargest now newith war tc two distruccp arddy in the f\n",
      "================================================================================\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 2100: 1.649657 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2200: 1.662953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2300: 1.644069 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2400: 1.649024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2500: 1.653410 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 2600: 1.644442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 2700: 1.621873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2800: 1.623128 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2900: 1.621686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 3000: 1.636763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "fultisms intelevels work a smohoracker and and claceinst popu a frest of again sw\n",
      "h moptives to zard theat to belgeral invent suzplies s for playingdut to resolnsi\n",
      "uo an ends hops deas so uweavo investic gold city to sepent s is the dide theodai\n",
      "jg with his a forms wilm englacified tour they the afletic ex called ounds relege\n",
      "xdegemesiament remaline see the antity undern milar but had commonsticau having s\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3100: 1.611580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3200: 1.629350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3300: 1.625767 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3400: 1.623470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3500: 1.612914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3600: 1.629935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 3700: 1.595202 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3800: 1.602724 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 3900: 1.586760 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 4000: 1.604876 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "gton apped in the inherforthern the have lists with of like that from of crite se\n",
      "ying of sequent a bay for a fwhen in gald y also two zero three generalamber been\n",
      "jnland to swing the straesion expressemeck furalbrary with th economination wide \n",
      "c version of our mathematics one y and kneedminest by five king jung ens the bran\n",
      "id swasid to the eyes estable scolderind wither some flag verially flander end aa\n",
      "================================================================================\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 4100: 1.621517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 4200: 1.598617 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 4300: 1.567352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4400: 1.591659 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.44\n",
      "Average loss at step 4500: 1.579846 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4600: 1.584294 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4700: 1.597102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 4800: 1.597276 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4900: 1.613589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 5000: 1.617901 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "qnse as of lab primaralt may blues about extents intellectmen two the forms event\n",
      "spana forus jpgheor by and the demanify doila is for as ut macture mix randean ge\n",
      "away do civy of only text to pant extencel albook macarchapplicipainition kartici\n",
      "qk and splaissaction armire izimal by between bces in by come leomic detch of cit\n",
      "ysters a zargestion tever chucd right gover three four imach artialiforted at cup\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 5100: 1.582753 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5200: 1.590594 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5300: 1.572584 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5400: 1.566044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5500: 1.556571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 5600: 1.545028 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5700: 1.576612 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 5800: 1.564017 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5900: 1.576633 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6000: 1.535285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "ia one mag with ibdist economic are volum indiotia returned an asting blaveseal t\n",
      "xnu for bible marx marince the missamis trresseums at cause such fall possiting p\n",
      "cum climanse famounts the more age the exclusine j britainen special isael of the\n",
      "gzrrade no levely mwactural termy been simple unite of puching a desterty ramns s\n",
      "ael theorely his micros with incomputer europed dedicated or mazing by one nine t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 6100: 1.584267 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6200: 1.579838 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 6300: 1.568889 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6400: 1.583931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6500: 1.576332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6600: 1.571291 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6700: 1.557700 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 6800: 1.575221 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 6900: 1.603802 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 7000: 1.585060 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "gdaled by the each broad be fave year in brand morgy fundom championship among ki\n",
      "dquering suppliy the were is goverthehingkin deone novel don paulur providen it i\n",
      "vps infliting in the hannadver of asia four zi many longton nations from drives o\n",
      "haber detation tyradcally andamather been as the first inting linited that americ\n",
      "nking ninoctrayed to coerada sovast it in alf the trade a pediacies is into she o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.59\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "              feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question c: Add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding layer: input, embedding output\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  # Keep probability for dropout\n",
    "  kp = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.nn.dropout(i, kp)\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    output = tf.nn.dropout(output_gate * tf.tanh(state), kp)\n",
    "    return output, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  single_char_inputs = train_data[:num_unrollings]\n",
    "  train_inputs = zip(single_char_inputs[:-1], single_char_inputs[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    index = tf.argmax(i[0], dimension=1) + tf.argmax(i[1], dimension=1) * vocabulary_size\n",
    "    embed = tf.nn.embedding_lookup(embeddings, index)\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  sample_input_index = tf.argmax(sample_input[0], dimension=1) + tf.argmax(sample_input[1], dimension=1) * vocabulary_size\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.319466 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.65\n",
      "================================================================================\n",
      "lrvsxt kdzweks twtq wzspihtm xns um e bpydoy t tcoenn  xwueggmyi   v  coaec hptpy\n",
      "gf t ym fbiqxk lth uisxtd tesa icpktrmte   abie nanfqt rwenvag imxevnutvizatee qm\n",
      "qrctuowsrlhs etxl zecfu tjtm   cn  pb oagugt  bu rhttrhwavmdcuhybiouf rltorsknebs\n",
      "nb  z urifytrzehvaee  ooqtcnabduneakoe zp a cnih mhbttbobsjnzocae am  oesuy  ne i\n",
      "iwlq ond egxweykleojineellmietxf htxcn yruns u o  uleo q q q etvdemlgekt teoleo  \n",
      "================================================================================\n",
      "Validation set perplexity: 19.65\n",
      "Average loss at step 100: 2.346409 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 200: 2.066781 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 300: 1.965051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 400: 1.940962 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.918492 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 600: 1.889878 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 700: 1.875546 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 800: 1.856050 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 900: 1.833080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 1000: 1.818528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "wylatemoniverson neted kuce deveru in eight ever straoinfmclusia to four the cons\n",
      "pxm langht occ empleasted to shoslations as of led hin otherly used of otterld we\n",
      " n ce the for numbeen it are one nine four one of some winch was on signist are t\n",
      "xvy date as greao also beod tare flarright ang tgo imate proaver the accoms krist\n",
      "mlets on the countriegan cention wel with to hoon which the settor the mahroname \n",
      "================================================================================\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 1100: 1.787153 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1200: 1.810223 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 1300: 1.785435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 1400: 1.784340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1500: 1.750364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1600: 1.759936 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 1700: 1.737042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1800: 1.741893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 1900: 1.746693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2000: 1.759081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "oes at quies they if yo some poweded a ceand clariectry two of azumin some for a \n",
      "nlez is engody boaties press orsical teogntain matter actions the  roir seorede s\n",
      "i aract follweror and state ratty a who leass thodo michris the united by zero ze\n",
      "hj enversino wish we beologisses with trated authrooatory general growim fording \n",
      "ects was palike unnoved to sbaties back cathoas for in not in as which showever a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 2100: 1.752023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2200: 1.714974 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2300: 1.692416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2400: 1.732566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 2500: 1.739715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2600: 1.735654 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2700: 1.759069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2800: 1.714349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 2900: 1.734119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 3000: 1.734749 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "ictireliment party the exist inteael mont found sing to einhas and havo and miniv\n",
      "cws relandlation of one of and and thine hard links jurse derformal equeenesse fa\n",
      "ebrogital and the raption constanch alt belism dibreges with lear martc in labtio\n",
      "djimblings in and eople calth and two ye them been f wolly or is logy teuropopus \n",
      "most such a such hights cloyii z ll was but australia turk of weaket longer the o\n",
      "================================================================================\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 3100: 1.727839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3200: 1.751900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 3300: 1.748056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3400: 1.787052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3500: 1.759469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3600: 1.772414 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3700: 1.765390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3800: 1.756197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3900: 1.742851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4000: 1.719707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ic farly greends a menser rograve belanguage the copany simplized constines romac\n",
      " be the and recorder medic write bot of dominuounies were procenuima seven a clai\n",
      "hs for jrunt scaternal to blue the temperine of the lat pronicinent foot and best\n",
      "afres roman and contron and one nine eight be who conforming chapen reased aboute\n",
      "dzselexcept and one eight eight five zero three fe be sfully and his deal from th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4100: 1.719226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4200: 1.738702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4300: 1.736601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4400: 1.759308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.15\n",
      "Average loss at step 4500: 1.760256 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 4600: 1.757826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4700: 1.764083 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4800: 1.752895 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4900: 1.736090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 5000: 1.782360 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "scotally palary st for  a n well tend human ich erssenation that tise promini esp\n",
      "rom sekna a partically and you huntilla and them well the chumatineminai likeroun\n",
      "qhublacted american the weere bearfarzaster doment domilative farled making s rec\n",
      "dnatain fought eptial hering atred wankachsnosoment christurs the gade divess tea\n",
      "funirced in from early willed manar dour stakserue from the are noccicy and the o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5100: 1.752816 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5200: 1.739086 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5300: 1.747484 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 5400: 1.715595 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 5500: 1.722042 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 5600: 1.723388 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5700: 1.738504 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5800: 1.718689 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5900: 1.728070 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6000: 1.696849 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "jr referated light origood greatt extres gronlisted the conceptmentasity avarc de\n",
      "xkau id been the prips emists polysic generary concular of japan laigries coponso\n",
      "bsoundrense the with while reasus hand three zero three two left between five thr\n",
      "jr trages deceudom decificondon reustrations there as ba expallcil rock while do \n",
      "xrry poly sice zope saistory bocoidea is appease toaq electrincil and prizecred f\n",
      "================================================================================\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6100: 1.714985 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 6200: 1.708018 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 6300: 1.731477 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 6400: 1.726797 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6500: 1.720778 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6600: 1.695065 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6700: 1.677135 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6800: 1.695292 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6900: 1.695074 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 7000: 1.696927 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "aw partion thie text to hafter cybritish one two eight seven zero two cish act is\n",
      "pqecint god is when intom when hus have kahowever pople fiops to bentinal syn for\n",
      "pvy gu dance in the humed in anothox nee five two one uniter verbian of links sho\n",
      "uqctioze him is usek with free christed ond one nine that januments rign channine\n",
      "acking of its of the kor have inously belision based in culm univer a three two t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      feed_dict[kp] = 0.9\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "              feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1], kp: 1.0})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1], kp: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cba'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'abc'\n",
    "b = a[::-1]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = text.split(' ')[1:]\n",
    "target_data = list()\n",
    "\n",
    "for i in source_data:\n",
    "    target_data.append(i[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['msihcrana',\n",
       " 'detanigiro',\n",
       " 'sa',\n",
       " 'a',\n",
       " 'mret',\n",
       " 'fo',\n",
       " 'esuba',\n",
       " 'tsrif',\n",
       " 'desu',\n",
       " 'tsniaga']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### refer from https://zhuanlan.zhihu.com/p/27608348\n",
    "\n",
    "def extract_character_vocab(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>', '<EOS>']\n",
    "    \n",
    "    set_words = list(set([character for line in data for character in line]))\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(special_words + set_words)}\n",
    "    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\n",
    "    \n",
    "    return int_to_vocab, vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_data)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_vocab(target_data)\n",
    "\n",
    "source_int = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) \n",
    "               for letter in line] for line in source_data]\n",
    "target_int = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) \n",
    "               for letter in line] + [target_letter_to_int['<EOS>']] for line in target_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6, 5, 24, 10, 9, 7, 14, 15],\n",
       " [4, 24, 7, 20, 7, 6, 5, 8, 13, 26],\n",
       " [5, 14],\n",
       " [5],\n",
       " [8, 13, 24, 15],\n",
       " [4, 19],\n",
       " [5, 22, 27, 14, 13],\n",
       " [19, 7, 24, 14, 8],\n",
       " [27, 14, 13, 26],\n",
       " [5, 20, 5, 7, 6, 14, 8]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15, 14, 7, 9, 10, 24, 5, 6, 5, 3],\n",
       " [26, 13, 8, 5, 6, 7, 20, 7, 24, 4, 3],\n",
       " [14, 5, 3],\n",
       " [5, 3],\n",
       " [15, 24, 13, 8, 3],\n",
       " [19, 4, 3],\n",
       " [13, 14, 27, 22, 5, 3],\n",
       " [8, 14, 24, 7, 19, 3],\n",
       " [26, 13, 14, 27, 3],\n",
       " [8, 14, 6, 7, 5, 20, 5, 3]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='lr')\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None, ), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name = 'max_target_length')\n",
    "    source_sequence_lenght = tf.placeholder(tf.int32, (None, ), name='source_sequence_length')\n",
    "    \n",
    "    return inputs, targets, learning_rate, target_sequence_length, max_target_sequence_length, source_sequence_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_layer(input_data, num_nodes, num_layers, source_sequence_length, source_vocab_size,\n",
    "                      embedding_size):\n",
    "    embeddings = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, embedding_size)\n",
    "    \n",
    "    def get_lstm_cell(num_nodes):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_nodes, initializer=tf.random_uniform_initializer(-0.1, 0.1, \n",
    "                                                                                                 seed=2))\n",
    "        \n",
    "        return lstm_cell\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(num_nodes) for _ in range(num_layers)])\n",
    "    \n",
    "    encoder_ouput, encoder_state = tf.nn.dynamic_rnn(cell, embeddings, sequence_length=source_sequence_length, \n",
    "                                                     dtype = tf.float32)\n",
    "    \n",
    "    return encoder_ouput, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(data, vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_layer(target_letter_to_int, decoding_embedding_size, num_layers, num_nodes, target_sequence_length,\n",
    "                     max_target_sequence_length, encoder_state, decoder_input):\n",
    "    \n",
    "    # Embedding target data\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)\n",
    "    \n",
    "    # Construst decoder\n",
    "    def get_decoder_cell(num_nodes):\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(num_nodes, initializer=tf.random_normal_initializer(-0.1, 0.1, \n",
    "                                                                                                   seed=2))\n",
    "        return decoder_cell\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(num_nodes) for _ in range(num_layers)])\n",
    "    output_layer = Dense(target_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, \n",
    "                                                                                               stddev=0.1))\n",
    "    \n",
    "    # Trainging part\n",
    "    with tf.variable_scope('decoder'):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input, \n",
    "                                                            sequence_length=target_sequence_length, \n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell, training_helper, encoder_state, output_layer)\n",
    "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, \n",
    "                                                                       impute_finished=True, \n",
    "                                                                       maximum_iterations=max_target_sequence_length)\n",
    "    \n",
    "    # Prediction part\n",
    "    with tf.variable_scope('decoder', reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], \n",
    "                              name ='start_tokens')\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    target_letter_to_int['<EOS>'])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                            predicting_helper,\n",
    "                                                            encoder_state,\n",
    "                                                            output_layer)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder, impute_finished=True,\n",
    "                                                                        maximum_iterations=max_target_sequence_length)\n",
    "    return training_decoder_output, predicting_decoder_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, lr, target_sequence_length, max_target_sequence_length, \n",
    "                 source_sequence_length, source_vocab_size, target_vocab_size, encoder_embedding_size, \n",
    "                  decoder_embedding_size, num_nodes, num_layers):\n",
    "    _, encoder_state = get_encoder_layer(input_data,\n",
    "                                        num_nodes,\n",
    "                                        num_layers,\n",
    "                                        source_sequence_length,\n",
    "                                        source_vocab_size,\n",
    "                                        encoder_embedding_size)\n",
    "    \n",
    "    decoder_input = process_decoder_input(target_data, target_letter_to_int, batch_size)\n",
    "    \n",
    "    training_decoder_output, predicting_decoder_output = get_decoder_layer(target_letter_to_int,\n",
    "                                                                          decoder_embedding_size,\n",
    "                                                                          num_layers,\n",
    "                                                                          num_nodes,\n",
    "                                                                          target_sequence_length,\n",
    "                                                                          max_target_sequence_length,\n",
    "                                                                          encoder_state,\n",
    "                                                                          decoder_input)\n",
    "    \n",
    "    return training_decoder_output, predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "num_nodes = 50\n",
    "num_layers = 50\n",
    "encoder_embedding_size = 15\n",
    "decoder_embedding_size = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    input_data, target_data, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_inputs()\n",
    "    \n",
    "    training_decoder_output, predicting_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      target_data, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoder_embedding_size, \n",
    "                                                                      decoder_embedding_size, \n",
    "                                                                      num_nodes, \n",
    "                                                                      num_layers)  \n",
    "    \n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='mask')\n",
    "    \n",
    "    with tf.name_scope('optimization'):\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(training_logits, target_data, masks)\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        gradients = optimizer.compute_gradients(loss)\n",
    "        clipped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i: start_i + batch_size]\n",
    "        targets_batch = targets[start_i: start_i + batch_size]\n",
    "        \n",
    "        padded_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        padded_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        targets_length = []\n",
    "        for target in targets_batch:\n",
    "            targets_length.append(len(target))\n",
    "        \n",
    "        sources_length = []\n",
    "        for source in sources_batch:\n",
    "            sources_length.append(len(target))\n",
    "        \n",
    "        yield padded_targets_batch, padded_sources_batch, targets_length, sources_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/60 Batch    0/132852 - Training Loss:  3.404  - Validation loss:  3.399\n",
      "Epoch   1/60 Batch   50/132852 - Training Loss:  2.813  - Validation loss:  2.808\n",
      "Epoch   1/60 Batch  100/132852 - Training Loss:  2.786  - Validation loss:  2.723\n",
      "Epoch   1/60 Batch  150/132852 - Training Loss:  2.726  - Validation loss:  2.700\n",
      "Epoch   1/60 Batch  200/132852 - Training Loss:  2.718  - Validation loss:  2.714\n"
     ]
    }
   ],
   "source": [
    "train_source = source_int[batch_size: ]\n",
    "train_target = target_int[batch_size: ]\n",
    "\n",
    "valid_source = source_int[:batch_size]\n",
    "valid_target = target_int[:batch_size]\n",
    "\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']))\n",
    "\n",
    "checkpoint = 'trained_seq2seq_model.ckpt'\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_length, sources_length) in enumerate(\n",
    "        get_batches(train_target, train_source, batch_size, \n",
    "                    source_letter_to_int['<PAD>'], \n",
    "                    target_letter_to_int['<PAD>'])):\n",
    "            \n",
    "            _, train_loss = sess.run(\n",
    "            [train_op, loss],\n",
    "            {input_data: sources_batch,\n",
    "             target_data: targets_batch,\n",
    "             lr: learning_rate,\n",
    "             source_sequence_length: sources_length,\n",
    "             target_sequence_length: targets_length})\n",
    "            \n",
    "            if batch_i % 50 == 0:\n",
    "                validation_loss = sess.run(\n",
    "                [loss],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 target_data: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 source_sequence_length: valid_sources_lengths,\n",
    "                 target_sequence_length: valid_targets_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Training Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              train_loss, \n",
    "                              validation_loss[0]))\n",
    "    \n",
    "    print('Train finished, the model is saved as %s'%checkpoint)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python3(tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
